# Introduction (Draft v1)

The rise of large language models has renewed old questions about interpretation. Can machines understand texts? Do they "read" in any meaningful sense? For digital humanists, these questions are not merely philosophical—they have practical consequences for how we design research workflows.

This paper argues that LLMs cannot interpret texts but can be productively integrated into human interpretive workflows. The key is maintaining clear boundaries: humans set the interpretive frame, AI contributes within that frame, and humans evaluate the results. This is not a limitation to work around but a design principle to embrace.

The digital hermeneutics literature provides theoretical resources for this argument, but requires updating. Foundational work by Capurro (2010) and recent contributions by Romele (2020) predate the current generation of generative AI. Their frameworks assume tools that analyze or retrieve—not tools that generate plausible text. [verify: check if any recent DH work addresses LLMs specifically]

I propose a four-phase workflow model...

---
**Status:** First draft, needs development
**Word count:** ~180 (target: 800)
